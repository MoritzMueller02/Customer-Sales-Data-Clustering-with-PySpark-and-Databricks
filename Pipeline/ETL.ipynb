{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ea27242-7baa-4691-a54b-3e53d28bc0ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer = spark.read.table(\"databricks_simulated_retail_customer_data.v01.customers\")\n",
    "sales = spark.read.table(\"databricks_simulated_retail_customer_data.v01.sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfef4471-bc3d-41b0-9a23-e6db4ff8be61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Customer Sales Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91d7be07-1619-4172-8645-332fc08133ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, col\n",
    "\n",
    "print(\"Total Customer \\n\")\n",
    "display(customer.count())\n",
    "print(\"\\n\")\n",
    "print(\"Total Distinct Customers \\n\")\n",
    "customer.select(countDistinct(col(\"customer_id\"))).show()\n",
    "print(\"\\n\")\n",
    "print(\"Number of Duplicates\")\n",
    "customer.select(customer.count() - countDistinct(col(\"customer_id\")).cast(\"int\")).alias(\"Diff\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b5fc2e-4ef8-48ab-a78e-2ecbbfb0462a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Findings on Missing Values\n",
    "\n",
    "During the analysis, we identified missing values in several key columns of the customer sales dataset. These gaps may impact the accuracy of sales insights and customer segmentation. The missing data is primarily concentrated in customer demographic fields and some transaction records. Addressing these missing values is recommended to ensure robust analysis and reliable reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a9c8f1a-3120-4f15-87b0-bf09bf61b664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "missing_ratio = customer.select([\n",
    "    (sum(col(c).isNull().cast(\"int\")) / customer.count()).alias(c)\n",
    "    for c in customer.columns\n",
    "]).toPandas().iloc[0]\n",
    "\n",
    "cols_2_drop = []\n",
    "for cols, val in missing_ratio.items():\n",
    "    if val > 0.00:\n",
    "        print(f\"{cols}: {val}\")\n",
    "        cols_2_drop.append(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a8201a1-145b-4e8f-8ced-8af4722b1c54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dupes = (customer.groupBy(\"customer_id\").count().filter(col(\"count\") > 1))\n",
    "customer_duplicates = customer.join(dupes.select(\"customer_id\"), on=\"customer_id\", how=\"inner\")\n",
    "customer_no_dupes = customer.subtract(customer_duplicates)\n",
    "display(customer_no_dupes.count())\n",
    "print(\"\\n\")\n",
    "display(customer.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47f91b29-30d1-4950-add1-71e40dc821a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_clean = customer_no_dupes.drop(*cols_2_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e12a43-7ba0-4024-88a0-629bda31b3ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_clean.select([\n",
    "    (sum(col(c).isNull().cast(\"int\"))/ customer_clean.count()).alias(c)\n",
    "    for c in customer_clean.columns\n",
    "]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46ce5b93-26c6-46e8-9563-0095870be87d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns_2_drop = [\n",
    " 'customer_name',\n",
    " 'state',\n",
    " 'postcode',\n",
    " 'street',\n",
    " 'region',\n",
    " 'ship_to_address',\n",
    " 'valid_from',\n",
    " ]\n",
    "\n",
    "customer_clean = customer_clean.drop(*columns_2_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4d19aa7-4310-4f8c-88bc-8b78a444a0dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_clean.write.mode(\"overwrite\").saveAsTable(\"default.customer_cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "815e91b4-341a-4705-9139-a754d16d7862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Engieering\n",
    "  The only viable information that we get is that we are in the USA. Hence we have 50 States, as the Regions Tab contains a lot of NANs, and it would be important for the actual feature engieeering to use regions to cluster. We drop the regions column, and create 50 clusters using KNN based on the features `lon` and `lat`. The column `states`contains the respective names of the states, however they dont posess any information about proximity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0fdf356-eab0-4053-b414-e86eb653d6b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_clean.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e10ebfa-d767-42af-ad58-ad27be12e627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, round\n",
    "\n",
    "customer_clean = customer_clean.withColumn(\"lat\", round(col(\"lat\"), 2))\n",
    "customer_clean = customer_clean.withColumn(\"lon\", round(col(\"lon\"), 2))\n",
    "\n",
    "result = customer_clean.groupBy(\n",
    "    \"lat\",\n",
    "    \"lon\"\n",
    ").agg(\n",
    "    count(\"*\").alias(\"lon_lat_count\")\n",
    ")\n",
    "display(result.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd7e9d0-77ce-46da-bca9-c0ce01ed84d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(\n",
    "    inputCols=[\"lat\", \"lon\"],\n",
    "    outputCol=\"lanlonVec\"\n",
    ")\n",
    "\n",
    "customer_clean = vecAssembler.transform(customer_clean)\n",
    "display(customer_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74c2bc91-17c9-4801-b13a-af519cd0a349",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "silhoutte_score = []\n",
    "\n",
    "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='lanlonVec', metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "\n",
    "for k in range(48,51):\n",
    "    kmeans = KMeans(featuresCol=\"lanlonVec\",k = k)\n",
    "    model = kmeans.fit(final_data)\n",
    "    predictions = model.transform(final_data)\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    silhoutte_score.append(silhouette)\n",
    "    print(\"Silhouette with k = {}: {}\".format(k, silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8682a74e-0009-4685-94ef-e6928a690918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.plot(range(48,51),silhoutte_score)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c72bff-0111-493e-9381-35424d7405f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(featuresCol=\"lanlonVec\",k = 50)\n",
    "model = kmeans.fit(customer_clean)\n",
    "predictions = model.transform(customer_clean)\n",
    "#customer_clean = customer_clean.withColumnRenamed(\"predictions\", \"cluster\")\n",
    "#customer_clean = customer_clean.drop(\"lanlonVec\", \"lat\", \"lon\")\n",
    "\n",
    "customer_final = predictions.select(\"customer_id\", \"prediction\", \"units_purchased\", \"loyalty_segment\")\n",
    "customer_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33a0659e-bbac-4d77-ac26-8f6710c5ef4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_clean.write.mode(\"overwrite\").saveAsTable(\"default.customer_cleaned_new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163f26dd-d8b9-4363-9c41-1d8aecd608fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Sales Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "221b7c3a-5d48-471a-9000-1585d48b4260",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a0ba8dc-7c93-43c4-8f69-ff775f9b4c5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_na = sales.select([\n",
    "    (sum(col(c).isNull().cast(\"int\")) / sales.count()).alias(c)\n",
    "    for c in sales.columns\n",
    "]).toPandas().iloc[0]\n",
    "\n",
    "sales_dupes = sales.select([\n",
    "    (countDistinct(col(c))).alias(c)\n",
    "    for c in sales.columns\n",
    "]).toPandas().iloc[0]\n",
    "\n",
    "print(\"Missing Values in Sales Dataframe\")\n",
    "print(sales_na)\n",
    "\n",
    "print(\"Duplicates in Sales Dataframe\")\n",
    "print(sales_dupes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da5456b3-acf4-4f97-a265-20393b47b6d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Sales Counts \\n\")\n",
    "display(sales.count())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Sales Distinct Count\\n\")\n",
    "display(sales.select(countDistinct(col(\"customer_id\"))).show())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Difference Customer\\n\")\n",
    "display(sales.count())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba805d20-dbe5-478d-bf73-c2292118515f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    sum, avg, count, countDistinct,\n",
    "    min, max, stddev, col, datediff, lit\n",
    ")\n",
    "\n",
    "customer_features = sales.groupBy(\"customer_id\").agg(\n",
    "    count(\"*\").alias(\"num_orders\"),\n",
    "    sum(\"total_price\").alias(\"total_spend\"),\n",
    "    avg(\"total_price\").alias(\"avg_order_value\"),\n",
    "    stddev(\"total_price\").alias(\"std_order_value\"),\n",
    "    countDistinct(\"product\").alias(\"num_distinct_products\"),\n",
    "    countDistinct(\"product_category\").alias(\"num_distinct_categories\"),\n",
    "    min(\"order_date\").alias(\"first_order_date\"),\n",
    "    max(\"order_date\").alias(\"last_order_date\")\n",
    ")\n",
    "\n",
    "\n",
    "customer_features = customer_features.withColumn(\n",
    "    \"customer_tenure_days\",\n",
    "    datediff(col(\"last_order_date\"), col(\"first_order_date\"))\n",
    ")\n",
    "\n",
    "\n",
    "max_date = sales.select(max(\"order_date\")).collect()[0][0] #collect returns col[0] row[0]  \n",
    "\n",
    "customer_features = customer_features.withColumn(\n",
    "    \"recency_days\",\n",
    "    datediff(lit(max_date), col(\"last_order_date\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d957fe72-cdfe-472d-be85-03dcf5e2b803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_features.toPandas().columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed7b051b-ba45-4f1d-a2d8-30729069642a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Merge the two dataframes to gain Informatin to only active customers\n",
    "\n",
    "- An inner join would be the best idea because it ensures that only customers present in both `customer_features` and `customer_final` are included in the merged dataset. This avoids introducing records with missing or incomplete information, resulting in a clean dataset for analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d8c8d5f-eee6-4390-b687-5174f8cafd6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clustering_df = customer_final.join(\n",
    "    customer_features,\n",
    "    on=\"customer_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "clustering_df.write.mode(\"overwrite\").saveAsTable(\"clustering_df\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6081388399983804,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
